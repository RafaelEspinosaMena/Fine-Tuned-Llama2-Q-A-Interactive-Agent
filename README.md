## Abstract
This project consists of training the Llama2 pretrained model on the provided data to answer questions. Below is the documentation for it, including decisions, method, results and observations. The code is separated into two, one that should not be run which trains the model, and the other which should be run by the user to run the interactive agent, called Llama2_UI.pdf. Further instructions can be found in instructions.pdf.
## Decisions 
I first chose to use the Llama 2 pretrained LLM by Meta as it is one of the most advanced models available. It also lends itself particularly well to finetuned programs as it has a commercial license available for free. I used a particular version called the Nous-Hermes version, which has 13 Billion parameters and is further finetuned than the base Llama2 model which serves our needs quite nicely. I chose to use the Gradient Ai API because they offer a way to finetune the 13B parameter Llama2 model without exceeding GPU or Ram availability. Originally, I had chosen to use the Hugging Face pipeline to download and train the model. However, that exceeded the memory capacity I was able to afford once I tried to develop the GUI, so it was infeasible.
## Method
To fine tune the model, I used Gradient Ai’s “fine tune” method which iteratively trains the model on the provided question-answer pairs in a JSON format. It uses a LoRA algorithm to do so, which reduces the computational complexity of the task and thus I am able to train it using a single computer. As for the evaluation of the fine tuning, I used RLFH (Reinforcement Learning from Human Feedback), which allows me to manually check the quality of the answers, which is hard to do using a more automated method.
After the model is trained, I developed a second jupyter notebook which downloads the tuned model from Gradient Ai’s online workspaces and loads up an interactive interface built using TKinter in Python.
## Results
After a long session of training, it became clear that the model performs best after about 20 epochs of training. It provides almost identical answers to those provided in most cases, and does not create additional answers that do not make sense. Lesser epochs cause the model to hallucinate, and use it’s prior knowledge to make up an answer or create inadequate responses. For example, with lesser epochs, the model completely ignored the first section of the answer structure (“According to chapter 1, etc”). On the other hand, more epochs create answers that simply replicate the training data.
## Observations
The Llama2 model is extremely powerful. While also testing GPT3.5-Turbo, Llama2 outperformed it in quality of answers most of the time as per y own testing. The fact that it is also free to use is astonishing, and an invaluable opportunity for commercial and academic players to utilize. I also noticed that using Gradient Ai to fine tune the model was a very cost- effective choice, as training it remotely on their servers is far more time and computationally efficient than training it on my own computer or a paid platform such as Google Collab or AWS.
For future testing, I would like to implement a more detailed numerical analysis of the finetuning process. However, as per the Gradient Ai docs, this capability is not yet available. I would also like to create a dedicated MongoDB database to store the training data, although that is not yet necessary due to the small nature of the dataset.
## Instructions
In order to use the interactive agent, please open the llama2_UI.ipynb jupyter notebook in a computer with an internet connection and at least 4GB of Ram. Then follow the steps outlined below. Note: the interactive agent (UI) will not run on Google Collab, but must be run locally due to the TKinter library.
Instructions to use the Interactive Agent:
1. Run the whole notebook.
Note: it may take a couple of minutes to download the required packages and the model depending on the speed of your internet connection.
2. Once the interactive UI opens up, type in your question in the top box.
Note: it takes a couple of seconds to initialize the model, please be patient and wait until you type in your question.
3. Click generate and wait for a response!
Note: The Llama2 model is large (13B parameters) so it may take up to 1 minute to generate a single response. Please be patient with it.
4. Close the interactive UI by clicking on the red x in the top corner. Once the GUI is closed, the program will end automatically. To reopen the UI, simply run the last box of the notebook again.
Note: the provided access token and workspace ID are for the purposes of this notebook only. They are connected to a single use account developed for this project and contain no useful information. They are simply required to download the model.
