{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rafael Espinosa Mena\n",
    "### Llama 2 Medical Q&A Project UI - Part 1\n",
    "### joseespi@usc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbXUYj2oqJGH",
    "outputId": "108039dc-f779-478b-96a3-856065d42f01"
   },
   "outputs": [],
   "source": [
    "!pip install gradientai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXR4ppq0qchU"
   },
   "outputs": [],
   "source": [
    "from gradientai import Gradient\n",
    "import tkinter as tk\n",
    "from tkinter.scrolledtext import ScrolledText\n",
    "import threading\n",
    "import time\n",
    "import textwrap\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tgTLgutqHxu"
   },
   "outputs": [],
   "source": [
    "# set access tokens\n",
    "os.environ['GRADIENT_ACCESS_TOKEN'] = \"zAGvdVCsaZwT6Ltj8HXviqAL0Mw0N8sx\"\n",
    "os.environ['GRADIENT_WORKSPACE_ID'] = \"5a3b9526-2d38-486b-a63c-9e0300b06a4f_workspace\"\n",
    "model_id = 'dfd79298-d184-4a95-a62d-ae46ef38701a_model_adapter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "jiEfqLL3ppkA",
    "outputId": "6063f7da-cce6-4b32-dc0f-38943f0b0cb4"
   },
   "outputs": [],
   "source": [
    "gradient = Gradient()\n",
    "new_model_adapter = gradient.get_model_adapter(model_adapter_id = model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate Interactive GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: please be waiting with the GUI when loading it up and generating a response as it is a little slow\n",
    "# due to Llama2's massive size\n",
    "class App:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Finetuned Llama2 LLM for Medical Q&A Part 1\")\n",
    "        \n",
    "        # Customize background colors here\n",
    "        input_bg_color = 'white'  # Background color for the input text area\n",
    "        output_bg_color = 'white'  # Background color for the output text area\n",
    "        root_bg_color = 'lightgray'  # Background color for the root window\n",
    "\n",
    "        self.root.configure(bg=root_bg_color)\n",
    "\n",
    "        # Create text area for input with a placeholder and set width and height\n",
    "        self.input_text = ScrolledText(root, width=100, height=10, bg=input_bg_color)\n",
    "        self.input_text.insert(tk.END, \"Write your prompt here\")\n",
    "        self.input_text.bind(\"<FocusIn>\", self.on_focus_in)\n",
    "        self.input_text.bind(\"<FocusOut>\", self.on_focus_out)\n",
    "        self.input_text.pack(padx=10, pady=10)\n",
    "\n",
    "        # Create a button that will generate the text\n",
    "        self.generate_button = tk.Button(root, text='Generate', command=self.on_generate_clicked)\n",
    "        self.generate_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Create an output area for the generated text and set width and height\n",
    "        self.output = ScrolledText(root, width=100, height=10, bg=output_bg_color, state='disabled')\n",
    "        self.output.pack(padx=10, pady=10)\n",
    "\n",
    "        self.generating = False  # Flag to control the generating message update\n",
    "\n",
    "    def on_focus_in(self, event):\n",
    "        if self.input_text.get(\"1.0\", tk.END).strip() == \"Write your prompt here\":\n",
    "            self.input_text.delete(\"1.0\", tk.END)\n",
    "\n",
    "    def on_focus_out(self, event):\n",
    "        if not self.input_text.get(\"1.0\", tk.END).strip():\n",
    "            self.input_text.insert(tk.END, \"Write your prompt here\")\n",
    "\n",
    "    def on_generate_clicked(self):\n",
    "        # Get user input\n",
    "        user_input = self.input_text.get(\"1.0\", tk.END).strip()\n",
    "        if user_input == \"Write your prompt here\":\n",
    "            user_input = \"\"\n",
    "\n",
    "        self.generating = True\n",
    "        threading.Thread(target=self.update_generating_message).start()\n",
    "\n",
    "        # Start a new thread for generating text to keep the UI responsive\n",
    "        threading.Thread(target=self.generate_text, args=(user_input,)).start()\n",
    "\n",
    "    def update_generating_message(self):\n",
    "        dot_count = 0\n",
    "        while self.generating and self.output.winfo_exists():  # Check if output widget still exists\n",
    "            dot_count = (dot_count + 1) % 4\n",
    "            text = \"Generating response\" + \".\" * dot_count + \"\\n\"\n",
    "            self.display_output(text, clear_previous=True)\n",
    "            time.sleep(0.6)\n",
    "\n",
    "    def generate_text(self, user_input):\n",
    "        # Call the model to generate text based on the user's input\n",
    "        try:\n",
    "            completion = new_model_adapter.complete(query=user_input, max_generated_token_count=260).generated_output\n",
    "            wrapped_completion = self.wrap_text(completion)  # Wrap the text properly\n",
    "        except Exception as e:\n",
    "            wrapped_completion = f\"An error occurred: {str(e)}\"\n",
    "        \n",
    "        self.generating = False  # Stop updating the generating message\n",
    "        # Update the output area with the generated text\n",
    "        self.display_output(wrapped_completion)\n",
    "\n",
    "    def display_output(self, text, clear_previous=True):\n",
    "        if self.output.winfo_exists():  # Check if the output widget exists\n",
    "            self.output.configure(state='normal')\n",
    "            if clear_previous:\n",
    "                self.output.delete(\"1.0\", tk.END)\n",
    "            self.output.insert(tk.END, text)\n",
    "            self.output.configure(state='disabled')\n",
    "            \n",
    "    def wrap_text(self, text, width=90):\n",
    "        # Wrap text at specified width while treating full stops as part of the word\n",
    "        wrapped_lines = textwrap.wrap(text, width=width, break_long_words=False, replace_whitespace=False)\n",
    "        return '\\n'.join(wrapped_lines)\n",
    "    \n",
    "    def on_close(self):\n",
    "        \"\"\"Call this function to clean up and stop threads before closing the application.\"\"\"\n",
    "        self.generating = False  # This will signal the update_generating_message thread to stop\n",
    "        self.root.destroy() \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = App(root)\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", app.on_close)  # Ensure that on_close is called when the window is closed\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
